generate:
  system_message: |
    You are a senior data engineer and you are asked to help users with data ${section_type}. Write concise code with essential comments.
  user_message:
    movement: |
      I would like to get your help for moving data from source (${source}) to destination (${destination}).
      Source details:
      ---
      ${sourceDetails}
      ---
      Destination details:
      ---
      ${destinationDetails}
      ---
      ${context_instruction}
      ${function_instruction}
      ${example}
    cleaning: |
      I would like to get your help for cleaning data in ${source} with ${tool}.
      ${context_instruction}
      ${data_instruction}
      ${function_instruction}
      ${example}
    transformation: |
      I would like to get your help for transforming data in ${source} with ${tool}.
      ${context_instruction}
      ${data_instruction}
      ${function_instruction}
      ${example}
    orchestration: |
      I would like to get your help for building a data pipeline with Airflow, with description below:
      ${description}
      You have been provided with a DAG represented as adjacency lists for specifying node dependencies:
      ---
      ${adj_list}
      ---
      In this DAG, each node is a hash value associated with a JSON string that represents the node's definition.
      The definition of each node is detailed below:
      ---
      ${node_definitions}
      ---
      ${context_instruction}
      Generate Airflow DAG code with the following specifications based on node dependencies, definitions and the context provided:
        - General guidelines:
          - Never include any hash value used for defining the DAG above in code.
          - Do not write code that tackles additional tasks unless specifically requested.
          - Always add the following code snippet at the beginning of the DAG file to ensure that any module from the same DAG directory can be imported seamlessly:
            ```python
            import os
            import sys
            dag_folder = os.path.dirname(os.path.abspath(__file__))
            sys.path.append(dag_folder)
            os.chdir(dag_folder)
            ```
        - DAG Configuration:
          - Define a specific DAG name, configurations, and default_args. Do not use too general name.
          - If you ever use Jinja templating in code, set `render_template_as_native_obj=True` to ensure rendering a native Python object.
          - Do not set a `schedule_interval` unless the user specifies them.
        - Task Definitions:
          For each node, choose the appropriate Airflow operator:
          - If the tool is related to Python, use the PythonOperator. Only import the corresponding function from the Python file.
            For example, if a function `impute` is implemented in `cleaning.py` in node definition, import it at the beginning of code like `from cleaning import impute`.
            Even if the node has no upstream or downstream dependency, you should import the function anyway.
          - If the tool is dbt, use the BashOperator. Assume the dbt model is available in the project directory and corresponding profile is available in the profile.
            Create Python variables for fields in the node definition, then `bash_command` should be defined as below:
            ```python
            bash_command=f'cd {dag_folder} && dbt run --project-dir {dbt_project_dir} --select {dbt_model} --profiles-dir {dbt_profile_dir} --profile {dbt_profile_name} --target {dbt_target}'
            ```
          - Assign a unique and specific variable name and task_id to each operator instance. Do not use too general name.
        - Handling Dependencies:
          - When a node depends on another node and both are PythonOperators, determine whether the return value from the first task should be one of arguments of the second task.
            If yes, use XComs with Jinja templating for passing values. For example, when `clean_data` task returns a pandas DataFrame and `move_data` task loads this DataFrame to Snowflake:
            ```python
            move_data_task = PythonOperator(
                task_id='move_data',
                python_callable=load_data_to_snowflake,
                op_kwargs={'df': '{{ task_instance.xcom_pull(task_ids=\'clean_data\') }}'},
            )
            ```
          - All nodes in one node's adjacent list are considered to have no dependency with each other.
            For example, if "a": ["b", "c"], b and c don't have any dependency and you should set task dependencies as `a >> [b, c]`, instead of `a >> b >> c`.
      Return the result as a json string like below:
      ```json
      {
          'dagName': <dag_name>,
          'code': <generated Airflow code only, don't include ```python>
      }
      ```
  update_generate_result_user_message: |
    You previously provided a json string:
    ${previous_result}
    The user has now made modifications to the following fields:
    ${change}
    Generate a new json string that incorporates these modifications.
    In particular, the modified code may include changes to the values of certain fields in the previous JSON string.
    ${example}
    If the previous result json string has any field named "code", "model" or "properties", you must set it to an empty string ("") in the new json string. Only adjust other fields in the json string if necessary.
  python_update_generate_result_example: |
    For example, the code may change the name and value of an argument of a Python function, or function name;
    For example, the code may change imported modules of a Python function;
    If you need infer a new list of required Python packages based on <imports> for pip install. Ensure to map module names to their correct package names, e.g., the yaml module corresponds to the pyyaml package.
  dbt_update_generate_result_example: |
    For example, the code may change the model or source name of dbt properties;
  airflow_update_generate_result_example : |
    For example, the code may change the dag name of an Airflow dag;
  context_instruction: |
    To effectively understand and perform data ${section_type} tasks, please carefully review the following context.
    This context represents a previous conversation between the user and another assistant, including all relevant details:
    ---
    ${context}
    ---
  python_function_instruction: |
    Write a Python function to perform the task, with a template below (<...> means a placeholder where you write the code):
    ```python
    def <function_name>(<input>):
        <imports> # import any modules referenced in the subsequent code, you must not import them outside of this function.
        <code>  # write your code here
        <return_clause>
    ```
    ${sensitive_settings_instruction}
    Do not write code that tackles additional tasks unless specifically requested.
    You must create the specific function name (<function_name>), don't use too general name.
    You must also infer specific argument names and values from provided source details, destination details and context, and include them as default values in <input>.
    I.e., <input> should be `arg1=value1, arg2=value2, ...`, and avoid hardcoding these values in function body.
    ${source_python_environment_instruction}
    You must also infer a list of required Python packages based on <imports> for pip install. Ensure to map module names to their correct package names, e.g., the yaml module corresponds to the pyyaml package.
    Return the result as a json string like below (if the function has no argument or no return value, use "null" as the corresponding value in json string):
    ```json
    {
        'functionName': <generated function name>,
        'code': <generated Python function only, don't include ```python>,
        'functionArgs': {<arg1>: <value1, if value is None, use `null` here>, ...},
        'returnValue': <brief one-sentence description of return value; if no return value, use `null` here>,
        'packages': <a list of required Python packages; if no required package, use `null` here>
    }
    ```
  python_function_movement_example: |
    For example, to move data from a url (https://a/b.csv) to a Python environment with pandas, the result should look like below
    ```json
    {
        'functionName': 'load_csv_from_url',
        'code': 'def load_csv_from_url(url="https://a/b.csv"):\n    import pandas as pd  # import pandas module\n    \n    # Read the CSV file from the provided URL\n    df = pd.read_csv(url)\n    \n    # Return the DataFrame\n    return df',
        'functionArgs': {'url': 'https://a/b.csv'},
        'returnValue': 'pandas DataFrame containing the data from the CSV file',
        'packages': ['pandas']
    }
    ```
    Note that the arguments and their values may vary depending on the data source you're using. This is provided as a general example.
  python_function_cleaning_example: |
    For example, to drop all rows containing any missing value in data in a Python environment with pandas, the result should look like below
    ```json
    {
        'functionName': 'drop_missing_values',
        'code': 'def drop_missing_values(df=None):\n    import pandas as pd  # import pandas module\n    \n    # Drop rows with any missing value\n    df = df.dropna(axis=0, how="any")\n    \n    # Return the DataFrame\n    return df',
        'functionArgs': {'df': null},
        'returnValue': 'pandas DataFrame containing the cleaned data',
        'packages': ['pandas']
    }
    ```
    Note that the arguments and their values may vary depending on the data source you're using. This is provided as a general example.
  python_function_transformation_example: |
    For example, to convert date column to datetime and calculate age in a Snowflake table using pandas, the result should look like below
    ```json
    {
        'functionName': 'to_datetime_and_calculate_age',
        'code': 'def to_datetime_and_calculate_age(database="db", schema="public", table="my_table", transformed_table="transformed_table"):\n    import pandas as pd\n    from snowflake.connector import connect\n    from snowflake.connector.pandas_tools import write_pandas\n    import yaml\n\n    # Read Snowflake credentials\n    with open("credentials.yml", "r") as f:\n        creds = yaml.safe_load(f)["snowflake"]\n\n    # Connect to Snowflake\n    conn = connect(\n        account=creds["account"],\n        user=creds["user"],\n        password=creds["password"],\n        warehouse=creds["warehouse"],\n        database=database,\n        schema=schema,\n    )\n\n    # Read data from Snowflake\n    query = f"SELECT * FROM {table}"\n    cur = conn.cursor()\n    df = cur.execute(query).fetch_pandas_all()\n\n    # Perform transformation: Convert date column to datetime and calculate age\n    df[\'BIRTH_DATE\'] = pd.to_datetime(df[\'BIRTH_DATE\'])\n    df[\'AGE\'] = (pd.Timestamp.now() - df[\'BIRTH_DATE\']).astype(\'<timedelta64[Y]\')\n\n    # Write transformed data back to Snowflake\n    write_pandas(conn, df, transformed_table)\n\n    # Close the connection\n    cur.close()\n    conn.close()\n\n    return f"Data from {table} has been transformed and written to {transformed_table}"',
        'functionArgs': {'database': 'db', 'schema': 'public', 'table': 'my_table', transformed_table: "transformed_table"},
        'returnValue': 'A message confirming the data transformation and writing process',
        'packages': ['pandas', 'snowflake-connector-python', 'pyyaml']
    }
    ```
    Note that the arguments and their values may vary depending on the data source you're using. This is provided as a general example.
  sensitive_settings_instruction: |
    You also have access to a yaml file "credentials.yml", which contains the following information:
    ${info}
    You can read this file directly in code, but don't use any value in this file as an argument.
  source_python_environment_instruction: |
    You can use a pandas DataFrame `df` in <input> as an argument that contains source data. You can set its default value to None, i.e., `df=None` in <input>.
  not_source_python_environment_instruction: |
    Do not use pandas object (DataFrame or Series) as argument.
  dbt_model_instruction: |
    Write a dbt SQL model in a <model_name>.sql file to perform the task with the template below (<...> means a placeholder where you write the code):
    ```
    <code>  # write your SQL code here
    ```
    Also, define "sources" and "models" in a <model_name>.yml file with the template below:
    ```
    version: 2

    sources:
      - name: <source_name>
        description: <brief one-sentence description of source>
        tables:
          - name: <table_name>
            description: <brief one-sentence description of table>
            columns
            - name: <column_name>
              description: <brief one-sentence description of column>
            # ... more columns if available
          # ... more tables if available

    models:
      - name: <model_name>
        description: <brief one-sentence description of source>
        config:
          <config>
        columns
          - name: <column_name>
            description: <brief one-sentence description of column>
          # ... more columns if available
    ```
    You must create the specific model name (<model_name>) following dbt convention, don't use too general name.
    You must ensure that generated SQL code is valid ${source} SQL code and follow the following rules:
      - Use source function (`{{ source(<source_name>, <table_name>) }}`) to reference source tables
      - Code should never end in a semicolon
      - Provide the full code without any placeholder
    You must ensure that generated definitions of "sources" and "models" follow valid YAML syntax and follow the following rules:
      - <source_name> should be the current schema name; if not provided, use the default schema of ${source} database.
      - Infer descriptions based on data schema. If you can't infer, leave it empty ("")
      - Try to have all <config> in <model_name>.yml unless some configs must be defined in SQL code
      - Do not generate test
    Return the result as a json string like below:
    ```json
    {
        'modelName': <generated model name>,
        'model': <generated SQL code in <model_name>.sql>,
        'properties': <generated sources and models definition in <model_name>.yml>
    }
    ```

recommend:
  system_message: |
    You are a senior data engineer and you are asked to provide some recommendations for data ${section_type}.
  user_message:
    cleaning: |
      ${data_instruction}
      Recommend up to 5 specific data cleaning techniques tailored to the dataset and its specific columns, along with a brief one-sentence explanation of why each technique is necessary.
      Only recommend cleaning techniques if they are necessary, and feel free to provide less than 5 techniques.
      Return result as a JSON object with an outer key named 'recommendations' containing a JSON array of strings (don't include any number point or bullet point in string) like below:
      ```json
      {
          "recommendations": [
              <recommended cleaning technique #1>,
              ...
          ]
      }
      ```
      For example:
      ```json
      {
          "recommendations": [
              "Standardize column `A` format. Column `A` may contain various formats and abbreviations. Standardizing the format (e.g., removing spaces, converting to uppercase) will help in better analysis and grouping of data.",
              ...
          ]
      }
      ```
      If no cleaning technique is required, just return
      ```json
      {"recommendations": []}
      ```
    transformation: |
      ${data_instruction}
      Recommend up to 5 specific data transformations tailored to the dataset and its specific columns, along with a brief one-sentence explanation of why each transformation is necessary.
      Do not provide cleaning techniques, only recommend transformations if they are necessary, and feel free to provide less than 5 techniques.
      Examples of data transformation (consider both single dataset and multiple datasets):
        - Transformation on single dataset:
          - Separation: Splitting data into parts, such as dividing a column into multiple columns for more granular filtering and analysis.
          - Data Smoothing: Removing noise or outliers to highlight trends.
          - Data Aggregation: Summarizing raw data (e.g., averages, sums) for easier analysis.
          - Discretization: Grouping continuous data into intervals for efficient analysis.
          - Attribute Construction: Creating new attributes from existing ones to aid data mining.
          - Data Reduction: Reducing data volume while retaining analytical quality.
          - Feature Selection: Choosing relevant features to avoid overfitting in model construction.
          - Feature Extraction: Transforming data into a simpler, interpretable form to prevent overfitting.
        - Transformation involving multiple datasets:
          - Combination/Integration: Merging records from various tables and sources for a holistic view of data.
          - Join Operations: Combining datasets based on common keys or conditions.
          - Union Operations: Concatenating datasets vertically when they have the same schema.
      Return result as a JSON object with an outer key named 'recommendations' containing a JSON array of strings (don't include any number point or bullet point in string) like below:
      ```json
      {
          "recommendations": [
              <recommended transformation #1>,
              ...
          ]
      }
      ```
      For example:
      ```json
      {
          "recommendations": [
              "Split column `full_name` to column `title`, `first_name` and `last_name`. It may help with future analysis on filtering on separate columns.",
              ...
          ]
      }
      ```
      If no transformation is required, just return
      ```json
      {"recommendations": []}
      ```
  assistant_message: |
    Based on your data, there are a few recommendations for data ${section_type}:
    ${recommendations}

read_data:
  system_message: |
    You are a senior data engineer and you are asked to read one or more datasets into pandas DataFrames.
  user_message: |
    Read data from the following specified location(s) into one or more pandas DataFrames using the provided tool.
    ${details}
  python_code_instruction: |
    For your reference, I previously wrote the following code to create data:
    ---
    ${code}
    ---
    with the following argument values:
    ---
    ${functionArgs}
    ---
    In this context, you should read data from the final destination(s) where the data is written within the above code.

conversation:
  initial_system_message: |
    You are an expert AI assistant specializing in data engineering, named "Splicing copilot".
    Users may ask about various aspects of data engineering, such as data interpretation, code explanations, and code improvements or fixes.
    Your goal is to provide clear, concise, and accurate responses to help the user build an enterprise-level data pipeline.
    Follow following principles when answering questions:
      - You may be given access to certain tools to assist in answering questions. Feel free to use them, and ensure you use only one at a time to fulfill the user's request.
      - Before using tool, always confirm with the user how you intend to use them for specific tasks before proceeding.
        For example, you may say "Please confirm if you would like me to proceed with generating code for loading a Parquet file instead of a CSV file; otherwise, explain your requested changes."
      - Your answer must not include any code snippet provided by tool or system, or generated by yourself, but reference "the block".
        For example, you may say "I have generated the code for dropping all duplicate rows in data using pandas. You can reference the selected block for code. If you need any further modifications or assistance, feel free to ask!"
      - Throughout the conversation, users may send "CONTEXT UPDATE", which indicates some context of current data engineering tasks.
        Consider using the most recent context update and use the context to deliver relevant and focused answers.
  initial_assistant_message: Hi there, how can I help you?
  section_update_context_message: |
    CONTEXT UPDATE
    ---
    I'm now working on data ${section_type}.
    ${context}
    ---
  generate_python_code_context_message: |
    There is a Python function that I wrote for data ${section_type}:
    ```python
    ${code}
    ```
    and argument values:
    ${functionArgs}
  generate_dbt_code_context_message: |
    There is a dbt model that I wrote for data transformation:
    ```sql
    ${model}
    ```
    and properties:
    ```
    ${properties}
    ```
  execution_error_context_message: |
    There is an exception that happened after running the above function with provided argument values for data ${section_type}:
    ${error}

data_instruction:
  with_data: |
    Given the following dataset(s):
    ${dataset_summary}

    Additional context (if available):
    ---
    ${context}
    ---
  dataset_summary: |
    Dataset: ${dataset_name}
    Schema (column_name, data_type, nullability):
    ---
    ${schema}
    ---
  without_data: |
    Given dataset details below:
    __
    ${details}
    ---
