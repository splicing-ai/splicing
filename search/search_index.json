{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Splicing is an open-source AI copilot for effortless data pipeline building. It offers the following key features:</p> <ul> <li>Notebook-style interface with chat capabilities in a web UI: Work on your data pipelines in a familiar Jupyter notebook interface, while the AI copilot assists and guides you by generating, executing, and debugging data engineering code throughout the process. </li> <li>No vendor lock-in: Build your data pipelines with any data stack of your choice, and select the LLM you prefer for your copilot, with full flexibility.</li> <li>Fully customizable: Break down your pipeline into multiple components\u2014such as data movement, transformation, and more\u2014and tailor each component to your specific needs. Splicing then seamlessly assembles these components into a complete, functional data pipeline.</li> <li>Secure and manageable: Host Splicing on your own infrastructure, with full control over your data and LLMs. Your data and secret keys are never shared with LLM providers at any time.</li> </ul>"},{"location":"#feature-highlights","title":"Feature Highlights","text":""},{"location":"#generate-code-with-conversations","title":"Generate Code with Conversations","text":"<p>Interact with Splicing Copilot to generate code seamlessly through natural language conversations.</p>    Your browser does not support the video tag."},{"location":"#edit-and-execute-code","title":"Edit and Execute Code","text":"<p>Modify your code within an interactive environment, similar to Jupyter notebooks, and execute it directly.</p>    Your browser does not support the video tag."},{"location":"#debug-code","title":"Debug Code","text":"<p>Request Splicing Copilot to debug your code and help identify and fix issues efficiently.</p>    Your browser does not support the video tag."},{"location":"#connect-pipeline-components-and-recommend-data-engineering-techniques","title":"Connect Pipeline Components and Recommend Data Engineering Techniques","text":"<p>When building a data pipeline, each component operates on the context and data from the previous one. Splicing Copilot understands this flow and recommends optimal data engineering techniques based on your pipeline's context.</p>    Your browser does not support the video tag."},{"location":"#export-code-ready-for-deployment","title":"Export Code Ready for Deployment","text":"<p>Download code for your data pipeline in a single click, optimized and ready for deployment.</p>"},{"location":"#download-code","title":"Download Code","text":""},{"location":"#code-structure","title":"Code Structure","text":""},{"location":"#ready-for-deployment","title":"Ready for Deployment","text":""},{"location":"#end-to-end-demo","title":"End-to-end Demo","text":"<p>The following video demonstrates how to use Splicing to build an end-to-end NYC taxi data pipeline:</p>"},{"location":"about/","title":"About","text":"<p>Please refer to our README for more information about Splicing.</p>"},{"location":"about/#feedback","title":"Feedback","text":"<p>If you have any feedback or suggestions, please feel free to open an issue in our repository, join our Discord server, or email info@splicing-ai.com.</p>"},{"location":"about/#contribute","title":"Contribute","text":"<p>We welcome contributions from everyone. Please refer to our CONTRIBUTING guide for more information.</p>"},{"location":"about/#faqs","title":"FAQs","text":"<p>Please refer to our FAQs for more information.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This is a high-level overview of Splicing's architecture. The \"User\" interacts with the \"Assistant\" through either a chat interface or a notebook interface. The \"Assistant\" is responsible for understanding user intent, answering questions, and invoking the appropriate tool to generate or debug code as needed. Code execution takes place on the backend.</p> <p>The underlying agent is built using LangGraph, supporting multi-turn conversations with access to code generation tools. In the future, we plan to integrate more tools to further enhance capabilities of the \"Assistant\".</p>"},{"location":"get-started/","title":"Getting Started","text":""},{"location":"get-started/#installation","title":"Installation","text":"<p>The easiest way to run Splicing is in Docker. After installing Docker, you can run Splicing by executing the following command: <pre><code>docker run -v $(pwd)/.splicing:/app/.splicing \\\n  -p 3000:3000 \\\n  -p 8000:8000 \\\n  -it --rm splicingai/splicing:latest\n</code></pre></p> <p>To install Splicing without Docker for development, please refer to the CONTRIBUTING guide.</p>"},{"location":"get-started/#setup","title":"Setup","text":"<p>After running the command above, navigate to http://localhost:3000/ to access the web UI of Splicing. You should see the following screen as the \"Home\" page:</p> <p></p> <p>By default, all application data is stored in the <code>./.splicing</code> folder within the current directory where you run the above command. If you want to persist the data, make sure to back up this folder.</p>"},{"location":"get-started/#usage","title":"Usage","text":"<p>For how to use Splicing, please refer to the How-to Guides.</p>"},{"location":"how-tos/copilot/","title":"Getting Assistance from Copilot","text":"<p>Splicing Copilot is a chat interface powered by Splicing's agentic system, designed to assist you in building data pipelines using natural language. You can choose the LLMs that power Splicing Copilot, tailoring it to your preferences.</p>"},{"location":"how-tos/copilot/#general-data-engineering-questions","title":"General Data Engineering Questions","text":"<p>Similar to ChatGPT, you can ask Splicing Copilot general data engineering questions regarding to your data pipelines, such as \"What is the difference between ETL and ELT?\" or \"What is the best way to transform data in Snowflake?\".</p>"},{"location":"how-tos/copilot/#recommendation","title":"Recommendation","text":"<p>After setting up a block in the data cleaning or data transformation section, Splicing Copilot will automatically recommend cleaning or transformation techniques based on your data's metadata (e.g., joining two tables on a common column). You can then select the most suitable techniques and ask Splicing Copilot to generate the corresponding code for you.</p>"},{"location":"how-tos/copilot/#code-generation","title":"Code Generation","text":"<p>You can also ask Splicing Copilot to generate code for your requested data engineering tasks in the current block. For example, you might ask, \"Remove all rows with null values in the age column\". Splicing Copilot will always confirm with you before generating any code (e.g., see this example).</p>"},{"location":"how-tos/copilot/#debugging","title":"Debugging","text":"<p>If your code doesn't work as expected and shows errors after execution, you can ask Splicing Copilot to help debug. Simply ask questions like \"Why isn't the code working?\" or \"Can you fix it for me?\". Splicing Copilot will analyze the error based on the current context, provide a detailed explanation, and confirm with you before generating new code to resolve the issue.</p>"},{"location":"how-tos/create-project/","title":"Creating a Project","text":"<p>A Project represents a data pipeline in Splicing. You can create a project by clicking the \"New Project\" button in the right top corner on \"Home\" page. An LLM is required to create a project, so you must add an LLM before creating a project.</p> <p>After creating a project, you can click the project name to navigate to the \"Project\" page.</p> <p></p> <p>The \"Project\" page consists of three parts:</p> <ul> <li>Sections: Different components of the data pipeline. For more details, please refer to the Building Data Pipeline with Sections guide.</li> <li>Notebook: A notebook-style workspace for writing and running code. For more details, please refer to the Writing and Running Code in Notebook-style Workspace guide.</li> <li>Copilot: A chat interface for interacting with Splicing copilot. For more details, please refer to the Getting Assistance from Copilot guide.</li> </ul> <p>Last but not least, you can click the \"Settings\" button in the left bottom corner to edit data integrations and LLMs at any time.</p>"},{"location":"how-tos/integration-llm/","title":"Adding Data Integrations and LLMs","text":"<p>By clicking \"Settings\" at the bottom of the left sidebar, you can navigate to the \"Settings\" page to add data integrations and LLMs for your projects. You can add or edit them at any time while working on a project.</p> <p></p>"},{"location":"how-tos/integration-llm/#data-integrations","title":"Data Integrations","text":"<p>You can add various data integrations into your data pipelines. For instance, you can configure BigQuery as the data warehouse in your pipeline. All keys and secrets are securely stored in the <code>./.splicing/credentials.yml</code> file and are never exposed to LLMs.</p> <p>The latest version of Splicing supports the following data integrations:</p> <ul> <li>Amazon S3</li> <li>DuckDB</li> <li>Google BigQuery</li> <li>Local Python Environment (data will be saved as a pandas DataFrame in the backend database)</li> </ul>"},{"location":"how-tos/integration-llm/#llms","title":"LLMs","text":"<p>You can add various LLMs to empower your copilot in building data pipelines. For example, you can add OpenAI GPT-4 as the LLM for your project. Please note that an LLM is required to create a project, so you must add at least one LLM before starting a new project.</p> <p>The latest version of Splicing supports the following LLM providers:</p> <ul> <li>OpenAI</li> </ul>"},{"location":"how-tos/notebook/","title":"Writing and Running Code in Notebook-style Workspace","text":"<p>You can work on your data pipeline just as you would write and run code in a Jupyter notebook. In Splicing, a Block functions like a cell in Jupyter notebook\u2014it is a unit of code that you can create and execute. You can simply create a new block by clicking the \"+\" button in the \"Notebook\" part of the \"Project\" page.</p>"},{"location":"how-tos/notebook/#setting-up-a-block","title":"Setting Up a Block","text":"<p>The first step in any data engineering task is identifying where your data is located and selecting the appropriate tools for processing it. Before writing any code, please provide the necessary information by clicking the button with three vertical dots in the top-right corner of a block:</p> <p></p> <p>You can modify the block's setup at any time while working on it. </p>"},{"location":"how-tos/notebook/#linking-a-source-block-in-setup","title":"Linking a Source Block in Setup","text":"<p>In the block setup, you can link a block from the current or other sections to the current block by configuring the optional \"Source Section\" and \"Source Block\" options. This indicates that the current block is built based on the output data from the source block, providing Splicing Copilot with the full context of your source block. For example, if you load data into specific tables in a data warehouse in the source block, Splicing Copilot will recognize where the data is stored and automatically use it in the current block. By linking blocks, you can create a seamless data pipeline without needing to manually track or specify data details.</p>"},{"location":"how-tos/notebook/#generating-code-in-a-block","title":"Generating Code in a Block","text":"<p>Splicing currently supports two popular programming languages for data engineering: Python and SQL with dbt. By simply clicking the \"Generate\" button (or request code generation through the conversation), Splicing will automatically generate the code needed to perform the requested data engineering tasks, based on the current block's configuration and your conversation with Splicing Copilot. Currently, generated code will overwrite any existing code in the block.</p> <p></p>"},{"location":"how-tos/notebook/#generating-python-code","title":"Generating Python Code","text":"<p>Unlike writing Python code in a Jupyter notebook cell, Splicing generates data engineering code within a block as a Python function. The Python function will follow the structure below (you should write your code in the same format):</p> <pre><code>def &lt;function-name&gt;(&lt;function-kwargs&gt;):\n    &lt;import-statement&gt;\n    &lt;data-engineering-code&gt;\n</code></pre> <p>where:</p> <ul> <li><code>&lt;function-name&gt;</code> will be the block name, representing the data engineering task the function performs.</li> <li><code>&lt;function-kwargs&gt;</code> will include key configurations for the data engineering task, typically defined in the block setup or provided through the conversation with Splicing Copilot.</li> <li><code>&lt;import-statement&gt;</code> will include the necessary import statements for third-party libraries required for the task.</li> <li><code>&lt;data-engineering-code&gt;</code> will be the main code performing the task.<ul> <li>Additionally, you can read secret keys from the <code>credentials.yaml</code> file.</li> <li>If the destination of your data movement task, or the location of your cleaning and transformation tasks, is the local Python environment, you should return a pandas DataFrame so Splicing can save it: <pre><code>def &lt;function-name&gt;(&lt;function-kwargs&gt;):\n  ... # import statements\n  ... # main data engineering code generating a pandas DataFrame `df`\n  return df\n</code></pre></li> <li>If the source of your data movement task, or if your current block is linked to a source block where data is output to the local Python environment, you should include <code>df=None</code> as a keyword argument with <code>None</code> as the default value in <code>&lt;function-kwargs&gt;</code>: <pre><code>def &lt;function-name&gt;(df=None, ...):  # more keyword arguments\n  ... # import statements\n  ... # main data engineering code taking a pandas DataFrame `df` from the source block\n</code></pre></li> </ul> </li> </ul> <p>The image above provides a concrete example of loading a CSV file from Amazon S3 to Google BigQuery. These conventions are designed to componentize data engineering tasks, enabling Splicing to easily build and assemble functional data pipelines. They also represent good practices in data engineering, promoting code readability and maintainability.</p>"},{"location":"how-tos/notebook/#generating-sql-dbt-code","title":"Generating SQL (dbt) Code","text":"<p>Currently, dbt is only supported in the data transformation section. Each code block will represent a dbt model, consisting of SQL code (Model) and YAML configuration (Properties), where:</p> <ul> <li>The model name will be the block name, representing the data transformation task the model performs.</li> <li>The properties file will include AI-generated descriptions of the source table, the model table, and their columns.</li> </ul> <p></p>"},{"location":"how-tos/notebook/#modifying-and-saving-code-in-a-block","title":"Modifying and Saving Code in a Block","text":"<p>The generated code may not always be perfect, so you can modify it to fix errors or adjust it to meet your needs if you prefer not to ask Splicing Copilot for changes. You can directly edit the code in the block and click the \"Save\" button to apply your changes. Keep in mind that if you change the function name in your Python code or the model name in your dbt model, the block name will be updated accordingly.</p>"},{"location":"how-tos/notebook/#running-code-in-a-block","title":"Running Code in a Block","text":"<p>You can run the code in a block by clicking the \"Run\" button at any time. After running the code, you may see three types of outputs:</p> <ul> <li>Data Preview: Data is a first-class citizen in Splicing. If your code executes successfully, Splicing Copilot will automatically read the data from the destination and display a sample of the data in the \"Data Preview\" tab (even with multiple destinations). However, note that this is based on Splicing Copilot's best effort, so the data might not be displayed if Splicing Copilot fails to read it from the destination.  </li> <li>Return Value: (Python only) The return value of your Python function, displayed if Splicing Copilot fails to read the data from the destination, even though your function executed successfully.</li> <li>Error: The error message and stack trace from the failed code execution. You can ask Splicing Copilot to help debug the error.</li> </ul>"},{"location":"how-tos/notebook/#setting-current-block","title":"Setting Current Block","text":"<p>Similar to a Jupyter Notebook, a section can contain multiple blocks. To help Copilot better understand the context of the requested data engineering task and generate code within a block, you can set or unset a block as the current block by clicking its name toggle if there are multiple blocks in a section. When you click the \"Generate\", \"Run\", or \"Save\" button for a block, it will automatically be set as the current block.</p>"},{"location":"how-tos/notebook/#downloading-code-for-building-data-pipeline","title":"Downloading Code for Building Data Pipeline","text":"<p>You can download all the code written by you and Splicing Copilot in the project by clicking the \"Download\" button in the top-right corner of the \"Project\" page. The code will be packaged as a zip file with the following directory structure:</p> <pre><code>&lt;project-name&gt;/\n\u251c\u2500\u2500 &lt;section-name&gt;/ (dbt project directory)\n\u251c\u2500\u2500 &lt;section-name&gt;.py (Python script containing functions defined in the blocks of a section)\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 dbt_profiles/\n\u2502   \u2514\u2500\u2500 profile.yml (dbt profile)\n\u251c\u2500\u2500 credentials.yml (all secret keys required for the project)\n\u2514\u2500\u2500 requirements.txt (all third-party Python libraries required for the project)\n</code></pre>"},{"location":"how-tos/orchestration/","title":"Assembling Sections and Orchestrating Data Pipeline","text":"<p>There should be only one orchestration section, and it must be the last section in the project, which assembles all previous sections and blocks into a complete and functional data pipeline.</p>"},{"location":"how-tos/orchestration/#using-airflow","title":"Using Airflow","text":"<p>Currently, Splicing supports using Apache Airflow to orchestrate data pipelines. By default, Splicing will generate an Airflow DAG in a block:</p> <ul> <li>The DAG name will be the block name, representing the data transformation task the model performs.</li> <li>Every block is defined as a task in the DAG.</li> <li>The dependencies between tasks will be defined if \"Source Section\" and \"Source Block\" are set in a block.</li> <li> <p>By default,</p> <ul> <li>A block written in Python will be defined as a PythonOperator. </li> <li>A block written in SQL (dbt) will be defined as a BashOperator, which runs <code>dbt run</code> command to run a dbt model.</li> </ul> <p>You can customize the task type (different operators) and the definition of the task (arguments to the operator) as your needs.</p> </li> </ul>"},{"location":"how-tos/orchestration/#exporting-data-pipeline-to-airflow","title":"Exporting Data Pipeline to Airflow","text":"<p>You can download the code in the project and export it as an Airflow DAG by following these steps:</p> <ul> <li>Install the project dependencies by running <code>pip install -r requirements.txt</code> in the Python environment where Airflow is running.</li> <li>Uncompress the downloaded zip file and place it in the <code>dags</code> folder of your Airflow home directory (i.e., <code>AIRFLOW_HOME/&lt;dags_folder&gt;</code>, usually <code>~/airflow/dags</code>), or upload it to a managed Apache Airflow service (e.g., MWAA or Astronomer).</li> <li>You should now be able to see the DAG in the Airflow UI and trigger a DAG run to execute the data pipeline.</li> </ul> <p></p>"},{"location":"how-tos/sections/","title":"Building Data Pipeline with Sections","text":"<p>Splicing componentizes the data pipeline into Sections, each serving a specific purpose, and then seamlessly assembles these components into a complete, functional data pipeline. Currently, Splicing supports the following types of sections:</p> <ul> <li>Movement: Move data from one source to another. For example, you can load CSV files from your S3 bucket to Google BigQuery for further transformation.</li> <li>Cleaning: Clean data to ensure it meets basic quality requirements before being used in downstream tasks. For example, this often includes processes like handling missing values, correcting data types, and removing duplicates to ensure consistency.</li> <li>Transformation: Transform data to meet the specific requirements of downstream tasks, such as data modeling and machine learning. This might involve aggregating data, creating new features, or converting formats.</li> <li>Orchestration: Orchestrate the components across different systems and processes within a data pipeline to ensure tasks are executed in the correct sequence and at the appropriate time. For more details, refer to the Assembling Sections and Orchestrating Data Pipeline guide.</li> </ul>"},{"location":"how-tos/sections/#creating-a-section","title":"Creating a Section","text":"<p>You can add a section by clicking the \"+\" button in the \"Sections\" part.</p> <p></p>"},{"location":"how-tos/sections/#working-on-a-section","title":"Working on a Section","text":"<p>Refer to the Writing and Running Code in Notebook-style Workspace guide to understand how to work on a section.</p>"}]}